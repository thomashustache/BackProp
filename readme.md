# BackProp
Implementation of the back propagation algorithm from scratch on a simple regression task

Notations:
$w_{i j}^{k}:$ weight for node $j$ in layer $l_{k}$ for incoming node $i$
$b_{i}^{k}:$ bias for node $i$ in layer $l_{k}$
$a_{i}^{k}:$ product sum plus bias (activation) for node $i$ in layer $l_{k}$.
$o_{i}^{k}:$ output for node $i$ in layer $l_{k}$
$r_{k}:$ number of nodes in layer $l_{k}$
$g$ : activation function for the hidden layer nodes
$g_{o}$ : activation function for the output layer nodes


![Screenshot](losses.png)

![Screenshot](estimations.png)


